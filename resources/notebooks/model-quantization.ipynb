{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LAjWVcVJ8FkO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuned Model Quantization for faster inferencing"
      ],
      "metadata": {
        "id": "LAjWVcVJ8FkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "id": "3VOuCNOL9VP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r llama.cpp/requirements.txt"
      ],
      "metadata": {
        "id": "_5CMeHBf9yw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login, snapshot_download"
      ],
      "metadata": {
        "id": "GUKsSmNvAad_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"oluwatobi-alao/llama2-hiring\"\n",
        "snapshot_download(repo_id=model_id, local_dir=\"llama.cpp/llama2-hiring\",\n",
        "                  local_dir_use_symlinks=False, revision=\"main\")"
      ],
      "metadata": {
        "id": "kM_eSYxE-1lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp"
      ],
      "metadata": {
        "id": "JbLLe59sCm0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!make --quiet"
      ],
      "metadata": {
        "id": "zW81NCvA9oyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert.py -h"
      ],
      "metadata": {
        "id": "05eGgBITCs1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"llama2-hiring\"\n",
        "# Convert to fp16\n",
        "fp16 = f\"{model_name}/{model_name}.fp16.bin\"\n",
        "!python convert.py {model_name} --outtype f16 --outfile {fp16}"
      ],
      "metadata": {
        "id": "LigUGGlPCvHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output"
      ],
      "metadata": {
        "id": "tOv3500ZFMMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUANTIZATION_METHODS = [\"q4_0\", \"q4_k_m\", \"q5_0\", \"q5_k_m\"]\n",
        "QUANTIZATION_METHODS = [\"q8_0\", \"q4_0\", \"q4_k_m\"]\n",
        "\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    output = f\"output/{model_name}.{method.upper()}.gguf\"\n",
        "    !./quantize {fp16} {output} {method}"
      ],
      "metadata": {
        "id": "uMbmUgTSDTMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lash output/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpzDXsoNOPUi",
        "outputId": "abed6acb-8c56-4941-b1fc-657106b122a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 15G\n",
            "4.0K drwxr-xr-x  2 root root 4.0K Dec 12 12:54 .\n",
            "4.0K drwxr-xr-x 21 root root 4.0K Dec 12 12:51 ..\n",
            "3.6G -rw-r--r--  1 root root 3.6G Dec 12 12:54 llama2-hiring.Q4_0.gguf\n",
            "3.9G -rw-r--r--  1 root root 3.9G Dec 12 13:02 llama2-hiring.Q4_K_M.gguf\n",
            "6.7G -rw-r--r--  1 root root 6.7G Dec 12 12:53 llama2-hiring.Q8_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "vmc6ptmDO4q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo, HfApi\n",
        "from google.colab import userdata\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "hf_token = input(\"Enter token\")\n",
        "\n",
        "api = HfApi()\n",
        "username = \"oluwatobi-alao\"\n",
        "\n",
        "# Create empty repo\n",
        "create_repo(\n",
        "    repo_id = f\"{username}/llama2-hiring-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=\"output/\",\n",
        "    repo_id=f\"{username}/llama2-hiring-GGUF\",\n",
        "    allow_patterns=f\"*.gguf\",\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "eMMSEhMzGy1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}